\chapter{Discussion}\label{sec:discussion}
\lhead{\emph{Discussion}}
% \section{Short summary of results (subsection without title)}
We have evaluated the visualization performance of the HmPPCAs model on several simulated and two real scRNA-seq data-sets. We then compared these results with those obtained using PPCA, t-SNE and UMAP. We found that HmPPCAs was most of the time able to improve upon PPCA. Since a PPCA is performed on the top-level data-set as the first step of HmPPCAs, it is always included within the HmPPCAs. Therefore, it was impossible for the HmPPCAs to score lower than the PPCA model. We also found that, although HmPPCAs improved over PPCA, it was not as accurate as t-SNE and UMAP. Both these techniques were in general able to create plots that separated the different cell types better than HmPPCAs could. UMAP scored highest of the two, but t-SNE performed almost equally well.

We saw that the accuracy achieved by the HmPPCAs was comparable when using VB and NUTS as inference method. For the PPCA model negligibles difference were observed, except in the case of the Darmanis data-set, where NUTS performed relatively weak compared to all other methods. The HmPPCAs scored equally well when using NUTS as when using VB on many of the data-sets.
% On some of the data-sets, however, especially on the complex Splatter data-sets, VB scored higher. This was also true for the Darmanis data-set, where VB scored even higher with its top-level PPCA than the whole HmPPCAs using NUTS, although its score did not improve anymore after adding more levels of depth.

% The reason for the difference in performance between using NUTS and VB is not clear. The PPCA produced plots that achieved higher visualization scores when using NUTS. This implies that cell types were separated better in these plots. Therefore one would expect that this would lead to better clustering, and therefore also more accurate results on deeper levels. This was not true, as soon as deeper levels were initialized, VB achieved higher accuracy scores on the visualization assay.

NUTS and VB were also compared in terms of computation time till convergence. VB was always faster, for both the PPCA and HmPPCAs models. Not only did VB return results faster, but the computation time also increased less steeply when adding more genes, indicating that the method is more scalable.

% \section{Implications (subsection without title)}

The HmPPCAs model improved visualization performance over PPCA. In most cases, it was able to separate the different cell types better than just a single top-level PPCA.
It should be noted though, that here, the level with the highest accuracy was taken to evaluate the performance of a HmPPCAs model. The visualization scores of our HmPPCAs model are therefore an upper bound of the models.


Another point for discussion is that our MoPPCAs model was biased to find a relatively high value of $\sigma^2$ and a lower value of $\bm{W}$, as discussed in Section \ref{sec:prios}. An expected value of $\sigma^2$ was based on the spread of the clusters in the latent space. Also, constraints were forced upon the value of $\sigma^2$. To evaluate the effect of this approach, the value of $\sigma^2$ as found by the MoPPCAs model and the value found by the EM-algorithm for PPCA when performed on the separate clusters were compared with the values at which the constraints were set. Upon evaluation, the lower bound of $\sigma^2$ was (barely) high enough to include the value of $\sigma^2$ as found by PPCA, and the MoPPCAs solution for $\sigma^2$ was very similar to the PPCA solution. Still, future implementations of the MoPPCAs model in Stan should use priors and constraints for $\sigma^2$ which are based on their actual expectation. For example, according to Bishop \& Tipping \cite{bishop1998hierarchical}, the maximum likelihood estimator of $\sigma^2$ is given by $\sigma^2_{ML} = \frac{1}{d-2}\sum_{j=3}^d \lambda_j$, where $\lambda_j$ is the $j$th eigenvalue of the covariance matrix of the cluster. Basing the expectation $\sigma^2$ on this approach would be more correct.

It should also be noted that the number of iterations used for NUTS (300) was relatively low, in an attempt to save computational cost. When $\pm$250 iterations were used, Stan would give warnings that the samples might not have converged, so 300 iterations were used to avoid these warnings. Still, 300 iterations are on the low side. We do see reasonable results that were largely comparable with the VB results, but it is possible that better results could have been achieved when using a higher number of iterations.

Lastly, the parameters of our interest (e.g. the latent data-set $Z$ and the responsibility terms $\gamma_i^k$) were traceable. However, the factor loadings matrix $W$ is more difficult to find, because $W$ may undergo any arbitrary rotation. Therefore, if a researcher is interested in $W$, it may be better to use the EM algorithm. Alternatively, they could base the result on a single sample and not the mean of multiple samples.

Despite these shortcomings of our HmPPCAs implementation, HmPPCAs was able to provide more information about the local structure within clusters. A top-level PPCA might show some clusters, but the addition of hierarchy reveals structures within these clusters. For example, in Figure \ref{fig:simple_250_vb}, the top-level PPCA would suggest that there are only four observable clusters of cells in the data-set. When taking the purple-geen cluster apart and visualizing its data-points in its own latent space, it becomes clear that this cluster actually consists of two sub-clusters which could not be separated by a top-level PPCA. Visualizing the latent space of the data-set gives a good idea of which data-points could form a distinct group. The latent space of clusters within the data-set provides more information on these clusters than the PPCA representation when all data is taken into account. When encountering clusters while using linear dimensionality reduction techniques, it may, therefore, reward to perform separate analyses on the clusters within the data-set.

Additionally, it is noteworthy that HmPPCAs does not necessarily separate cell types, but specifically cluster structures in the data. Figure \ref{fig:nestorowa_nuts} shows how one group, the progenitor cells, shows different clusters. Each of those clusters is taken separately for further analysis, even though they belong to the same group of cells. This is not necessarily a bad thing and it is possible that there are differences to be found in the cells belonging to this type. Therefore, it could be the case that even subsets of the data-set that belong to the same type of cells need different analyses in terms of dimensionality reduction.

When comparing the HmPPCAs with UMAP and t-SNE however, it did not reach their visualization performance quite yet. It seems therefore that UMAP and t-SNE are non-linear approaches that are better at separating groups within the data than PPCA, even when extended to HmPPCAs. This might be due to the automatic clustering since the interactive EM implementation of HmPPCAs has been found to improve on t-SNE \cite{philipsthesis}. They may therefore have the advantage in the visualization assay that was performed.

% However, HmPPCAs has its own advantages, as it gives a more accurate representation of the real structure of the data-set, even though this does not necessarily separate groups within the data structure.

% \section{Future research (subsection without title)}

% \section{Final conclusion(s) (subsection without title)}
All in all, extending PPCA to a hierarchical model is a good way to obtain more insight into the local structure of the data-set. It might not be as good as UMAP or t-SNE to separate different structures within the data, but it is a valuable extension to linear dimensionality reduction techniques. Also, it seems that VB did not perform significantly worse than NUTS and possibly even better on some models. VB also took less time to converge and showed better scalability when adding more genes. The ADVI algorithm has therefore proven to be of great value when using Stan.