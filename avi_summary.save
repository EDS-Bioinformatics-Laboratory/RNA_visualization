ADVI (automatic differentiation variational inference) works through these steps:

- The user inputs a model written in Stan
- ADVI transforms the support of the function to include only real-valued variables
- ADVI then minimizes KL(q(theta)||p(theta|x))
- ADVI recasts the gradient of the KL as an expectation over q
- ADVI reparameterizes the gradient in terms of a Gaussian by a transformation
- ADVI uses noisy gradients to optimize variational distribution

- minimize KL = maximize ELBO
- transform by using Weibull and Poisson distribution
- maximize elbo by stochastic gradient ascent
	- automatic differentiation to compute gradient
	- MC integration to approximate expectations
- elliptical standardization to make expectation known

